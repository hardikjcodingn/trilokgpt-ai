# TrilokGPT Configuration

# Server Port
PORT=8000

# API URL (for frontend to know where to call backend)
API_URL=http://localhost:8000

# Ollama Configuration
# Make sure Ollama is running: ollama serve
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Embedding Model
# Other options: nomic-embed-text, all-minilm-l6-v2, neural-coil-mxbai-embed-large
EMBEDDING_MODEL=nomic-embed-text

# File Upload Configuration
UPLOADS_DIR=./uploads
VECTOR_STORE_PATH=./vectors/store.json

# Logging
LOG_LEVEL=info
