# TrilokGPT Example Configuration
# Copy this file to .env and customize for your setup

# ============================================
# SERVER CONFIGURATION
# ============================================

# Port to run the server on
PORT=8000

# Frontend API URL (used by browser to call backend)
# For local development: http://localhost:8000
# For network access: http://<YOUR_IP>:8000
# For production: https://yourdomain.com
API_URL=http://localhost:8000

# ============================================
# OLLAMA CONFIGURATION
# ============================================

# Ollama service URL
# Default: http://localhost:11434
# Note: Make sure Ollama is running with: ollama serve
OLLAMA_URL=http://localhost:11434

# Ollama model to use for LLM responses
# Options: llama2, mistral, gemma, neural-chat, openchat, etc.
# Download with: ollama pull <model-name>
OLLAMA_MODEL=llama2

# ============================================
# EMBEDDING CONFIGURATION
# ============================================

# Embedding model for document vectorization
# Options:
#   - nomic-embed-text (384-dim, fast) [DEFAULT]
#   - all-minilm-l6-v2 (384-dim, small)
#   - all-mpnet-base-v2 (768-dim, larger)
#   - mxbai-embed-large (1024-dim, best accuracy)
# Download with: ollama pull <model-name>
EMBEDDING_MODEL=nomic-embed-text

# ============================================
# FILE STORAGE CONFIGURATION
# ============================================

# Directory to store uploaded files
UPLOADS_DIR=./uploads

# Path to save vector store (FAISS database)
VECTOR_STORE_PATH=./vectors/store.json

# Maximum file size for uploads (in bytes)
# Default: 524288000 (500MB)
MAX_FILE_SIZE=524288000

# ============================================
# LOGGING CONFIGURATION
# ============================================

# Log level: debug, info, warn, error
LOG_LEVEL=info

# ============================================
# ADVANCED CONFIGURATION
# ============================================

# Number of chunks to retrieve for similarity search (default: 5)
TOP_K_RESULTS=5

# Chunk size in tokens (approximately 4 chars = 1 token)
CHUNK_SIZE=500

# Chunk overlap in tokens
CHUNK_OVERLAP=50

# Vector similarity threshold (0-1)
SIMILARITY_THRESHOLD=0.3

# ============================================
# NOTES
# ============================================

# 1. Make sure Ollama is running:
#    ollama serve
#
# 2. Pull models before first run:
#    ollama pull llama2
#    ollama pull nomic-embed-text
#
# 3. For production, use absolute paths or environment-based paths
#
# 4. Keep this file secure - it may contain sensitive information
#
# 5. Don't commit .env to version control
